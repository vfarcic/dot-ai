## Local Embedding Service

Optional in-cluster embedding service that provides zero-config semantic search without external API keys. Previously, semantic search (patterns, policies, capabilities, knowledge base) required configuring an OpenAI, Google, or Amazon Bedrock API key for embeddings — adding setup friction for new users, blocking air-gapped deployments, and incurring per-token costs.

Setting `localEmbeddings.enabled: true` in Helm values deploys a [HuggingFace Text Embeddings Inference (TEI)](https://github.com/huggingface/text-embeddings-inference) service alongside dot-ai. The service uses the `all-MiniLM-L6-v2` model (384 dimensions, ~256 MB RAM, no GPU required) and exposes an OpenAI-compatible API that dot-ai connects to automatically. Existing users with cloud embedding API keys configured see no behavior change — their setup continues working as before.

A new REST API endpoint (`POST /api/v1/embeddings/migrate`) handles switching between embedding providers by re-embedding all stored data. It migrates individual collections or all collections at once, skips collections that already match the target dimensions, and reports per-collection progress. The endpoint is also available via the auto-generated CLI (`dot-ai embeddings migrate`).

CI tests now run with local embeddings on amd64 GitHub Actions runners, removing the `OPENAI_API_KEY` dependency from CI. Local development on ARM/Apple Silicon continues using OpenAI embeddings.

See the [Embedding Provider Configuration](https://devopstoolkit.ai/docs/mcp/setup/deployment#embedding-provider-configuration) for setup details, cloud provider options, and migration instructions.
