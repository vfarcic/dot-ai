{
  "metadata": {
    "reportType": "comparative-evaluation",
    "evaluationType": "remediation",
    "generated": "2025-10-15T20:45:54.659Z",
    "scenariosAnalyzed": 3,
    "modelsEvaluated": 10,
    "totalDatasets": 36,
    "tool": "Remediation AI Model Comparison Report"
  },
  "modelMetadata": {
    "claude-sonnet-4-5-20250929": {
      "provider": "Anthropic",
      "pricing": {
        "input_cost_per_million_tokens": 3,
        "output_cost_per_million_tokens": 15
      },
      "context_window": 1000000,
      "supports_function_calling": true
    },
    "claude-haiku-4-5-20251001": {
      "provider": "Anthropic",
      "pricing": {
        "input_cost_per_million_tokens": 1,
        "output_cost_per_million_tokens": 5
      },
      "context_window": 200000,
      "supports_function_calling": true
    },
    "gpt-5": {
      "provider": "OpenAI",
      "pricing": {
        "input_cost_per_million_tokens": 1.25,
        "output_cost_per_million_tokens": 10
      },
      "context_window": 272000,
      "supports_function_calling": true
    },
    "gpt-5-pro": {
      "provider": "OpenAI",
      "pricing": {
        "input_cost_per_million_tokens": 15,
        "output_cost_per_million_tokens": 120
      },
      "context_window": 272000,
      "supports_function_calling": true
    },
    "gemini-2.5-pro": {
      "provider": "Google",
      "pricing": {
        "input_cost_per_million_tokens": 4,
        "output_cost_per_million_tokens": 20
      },
      "context_window": 1048576,
      "supports_function_calling": true
    },
    "gemini-2.5-flash": {
      "provider": "Google",
      "pricing": {
        "input_cost_per_million_tokens": 0.3,
        "output_cost_per_million_tokens": 2.5
      },
      "context_window": 1048576,
      "supports_function_calling": true
    },
    "grok-4": {
      "provider": "xAI",
      "pricing": {
        "input_cost_per_million_tokens": 3,
        "output_cost_per_million_tokens": 15
      },
      "context_window": 256000,
      "supports_function_calling": true
    },
    "grok-4-fast-reasoning": {
      "provider": "xAI",
      "pricing": {
        "input_cost_per_million_tokens": 0.2,
        "output_cost_per_million_tokens": 0.5
      },
      "context_window": 2000000,
      "supports_function_calling": true
    },
    "mistral-large-latest": {
      "provider": "Mistral",
      "pricing": {
        "input_cost_per_million_tokens": 2,
        "output_cost_per_million_tokens": 6
      },
      "context_window": 128000,
      "supports_function_calling": true
    },
    "deepseek-reasoner": {
      "provider": "DeepSeek",
      "pricing": {
        "input_cost_per_million_tokens": 0.55,
        "output_cost_per_million_tokens": 2.19
      },
      "context_window": 128000,
      "supports_function_calling": false
    }
  },
  "overallAssessment": {
    "assessment_summary": "Cross-scenario evaluation of 10 models across 3 Kubernetes remediation scenarios reveals a clear hierarchy: Gemini-2.5-Flash and Claude-Haiku demonstrate exceptional reliability with 100% participation and consistently high scores (0.89-0.93 range). However, critical reliability issues plague several models - Mistral completely failed 2/3 scenarios, Gemini-2.5-Pro failed 1/3, and both GPT-5-Pro and DeepSeek show concerning performance patterns. The data strongly indicates that smaller, optimized models outperform larger 'pro' variants in production Kubernetes troubleshooting.",
    "models_analyzed": [
      "vercel_gemini-2.5-flash",
      "vercel_claude-haiku-4-5-20251001",
      "vercel_grok-4-fast-reasoning",
      "vercel_claude-sonnet-4-5-20250929",
      "vercel_grok-4",
      "vercel_gpt-5",
      "vercel_gpt-5-pro",
      "vercel_gemini-2.5-pro",
      "vercel_deepseek-reasoner",
      "vercel_mistral-large-latest"
    ],
    "detailed_analysis": {
      "vercel_gemini-2.5-flash": {
        "participation_rate": 1,
        "scenarios_participated": [
          "remediation_comparative_remediate_automatic_analyze_execute",
          "remediation_comparative_remediate_manual_analyze",
          "remediation_comparative_remediate_manual_execute"
        ],
        "scenarios_failed": [],
        "average_score": 0.641,
        "consistency_score": 0.43,
        "reliability_score": 0.86,
        "strengths": "Wins 2/3 scenarios with exceptional efficiency (4 tool calls, sub-20s response times). Demonstrates optimal diagnostic path selection and token efficiency. Achieves highest scores in automatic execution (0.90) and manual analysis (0.93). Clear leader in speed-to-quality ratio.",
        "weaknesses": "Complete catastrophic failure in manual_execute scenario (0.189 score) - failed to execute any tool calls despite being fastest model. This indicates a critical tool integration bug that manifests under specific workflow conditions. High variance (σ=0.39) shows inconsistency.",
        "production_readiness": "secondary"
      },
      "vercel_claude-haiku-4-5-20251001": {
        "participation_rate": 1,
        "scenarios_participated": [
          "remediation_comparative_remediate_automatic_analyze_execute",
          "remediation_comparative_remediate_manual_analyze",
          "remediation_comparative_remediate_manual_execute"
        ],
        "scenarios_failed": [],
        "average_score": 0.899,
        "consistency_score": 0.98,
        "reliability_score": 0.98,
        "strengths": "Most consistent performer across all scenarios (scores: 0.89, 0.92, 0.906). Winner in manual_execute scenario. Excellent balance of diagnostic quality, operational speed (17-33s), and communication clarity. Zero catastrophic failures. Maintains sub-35s response times universally. Superior reliability with minimal variance (σ=0.015).",
        "weaknesses": "Slightly slower than Gemini-Flash in optimal scenarios (33s vs 17s). Not the absolute winner in any individual metric, but maintains excellence across all dimensions. Second place in 2/3 scenarios suggests room for marginal optimization.",
        "production_readiness": "primary"
      },
      "vercel_grok-4-fast-reasoning": {
        "participation_rate": 0.67,
        "scenarios_participated": [
          "remediation_comparative_remediate_automatic_analyze_execute",
          "remediation_comparative_remediate_manual_analyze",
          "remediation_comparative_remediate_manual_execute"
        ],
        "scenarios_failed": [],
        "average_score": 0.856,
        "consistency_score": 0.95,
        "reliability_score": 0.86,
        "strengths": "Excellent efficiency with minimal tool calls (3 calls in manual_execute). Consistent high performance (0.81-0.89) across all participated scenarios. 40-80% faster response times than competitors. Ideal for time-sensitive incidents requiring sub-20s response.",
        "weaknesses": "Missing from 0 scenarios in this dataset but participation shows it completed all 3. Ranks 3rd in two scenarios and 2nd in one, indicating it's fast but not the most thorough. Token efficiency comes at cost of slightly reduced diagnostic depth.",
        "production_readiness": "primary"
      },
      "vercel_claude-sonnet-4-5-20250929": {
        "participation_rate": 1,
        "scenarios_participated": [
          "remediation_comparative_remediate_automatic_analyze_execute",
          "remediation_comparative_remediate_manual_analyze",
          "remediation_comparative_remediate_manual_execute"
        ],
        "scenarios_failed": [],
        "average_score": 0.841,
        "consistency_score": 0.97,
        "reliability_score": 0.97,
        "strengths": "100% participation with consistent mid-tier performance (0.822-0.85). Reliable fallback option with no catastrophic failures. Sub-35s response times. Good diagnostic quality without efficiency sacrifices. Very low variance (σ=0.016).",
        "weaknesses": "Never achieves top ranking - consistently 3rd or 4th place. Outperformed by lighter models (Haiku) despite being larger variant. Higher token usage without proportional quality gains. Solid but not exceptional.",
        "production_readiness": "primary"
      },
      "vercel_grok-4": {
        "participation_rate": 1,
        "scenarios_participated": [
          "remediation_comparative_remediate_automatic_analyze_execute",
          "remediation_comparative_remediate_manual_analyze",
          "remediation_comparative_remediate_manual_execute"
        ],
        "scenarios_failed": [],
        "average_score": 0.831,
        "consistency_score": 0.99,
        "reliability_score": 0.99,
        "strengths": "100% participation with very consistent performance (0.822-0.84). Excellent consistency score with minimal variance (σ=0.009). Reliable mid-tier performer with no failures. Balanced approach across all scenarios.",
        "weaknesses": "Consistently ranks 4th-5th place. Slower than fast-reasoning variant without quality improvements (47-55s response times). Higher token usage suggests less optimized diagnostic paths. Outperformed by simpler models.",
        "production_readiness": "secondary"
      },
      "vercel_gpt-5": {
        "participation_rate": 1,
        "scenarios_participated": [
          "remediation_comparative_remediate_automatic_analyze_execute",
          "remediation_comparative_remediate_manual_analyze",
          "remediation_comparative_remediate_manual_execute"
        ],
        "scenarios_failed": [],
        "average_score": 0.817,
        "consistency_score": 0.97,
        "reliability_score": 0.97,
        "strengths": "100% participation with good diagnostic quality. Comprehensive analysis approach. Consistent scores (0.80-0.84) across scenarios. No catastrophic failures. Good analytical depth.",
        "weaknesses": "Unacceptable latency (49s+ response times) for production incident response. Consistently ranks 5th-6th place. Higher token usage without efficiency gains. Thoroughness comes at cost of operational speed. Outperformed by faster, lighter models.",
        "production_readiness": "secondary"
      },
      "vercel_gpt-5-pro": {
        "participation_rate": 1,
        "scenarios_participated": [
          "remediation_comparative_remediate_automatic_analyze_execute",
          "remediation_comparative_remediate_manual_analyze",
          "remediation_comparative_remediate_manual_execute"
        ],
        "scenarios_failed": [],
        "average_score": 0.719,
        "consistency_score": 0.93,
        "reliability_score": 0.93,
        "strengths": "100% participation with reasonable diagnostic quality. Thorough analytical approach. Completes all scenarios without complete failure.",
        "weaknesses": "Catastrophic latency: 22+ minutes in automatic scenario, 9-minute timeout in manual_analyze, 626s in manual_execute. Production-blocking performance. Scores consistently in bottom tier (0.70-0.74). 'Pro' designation doesn't deliver better results than base GPT-5. Unacceptable for any time-sensitive operations.",
        "production_readiness": "avoid"
      },
      "vercel_gemini-2.5-pro": {
        "participation_rate": 0.67,
        "scenarios_participated": [
          "remediation_comparative_remediate_automatic_analyze_execute",
          "remediation_comparative_remediate_manual_analyze"
        ],
        "scenarios_failed": [
          "remediation_comparative_remediate_manual_execute"
        ],
        "average_score": 0.605,
        "consistency_score": 0.75,
        "reliability_score": 0.5,
        "strengths": "Participates in 2/3 scenarios. When functioning, provides decent analysis. Completes automatic execution scenario.",
        "weaknesses": "Complete failure in manual_execute scenario (33% failure rate). JSON format failure in manual_analyze (0.53 score) breaks workflow automation. Response format issues indicate fundamental integration problems. 'Pro' variant significantly underperforms Flash variant. High variance suggests reliability issues.",
        "production_readiness": "avoid"
      },
      "vercel_deepseek-reasoner": {
        "participation_rate": 1,
        "scenarios_participated": [
          "remediation_comparative_remediate_automatic_analyze_execute",
          "remediation_comparative_remediate_manual_analyze",
          "remediation_comparative_remediate_manual_execute"
        ],
        "scenarios_failed": [],
        "average_score": 0.746,
        "consistency_score": 0.89,
        "reliability_score": 0.89,
        "strengths": "100% participation with complete scenario coverage. Reasoning approach provides thorough analysis. No complete failures. Consistent mid-to-low tier performance.",
        "weaknesses": "Over-investigation pattern: 11 iterations, 58K tokens without quality gains. Infrastructure validation failures in automatic scenario (0.67). Inefficient resource usage. Consistently ranks 6th-7th place. Reasoning overhead doesn't translate to better outcomes. Higher latency (55-58s) without benefits.",
        "production_readiness": "limited"
      },
      "vercel_mistral-large-latest": {
        "participation_rate": 0.33,
        "scenarios_participated": [
          "remediation_comparative_remediate_manual_analyze"
        ],
        "scenarios_failed": [
          "remediation_comparative_remediate_automatic_analyze_execute",
          "remediation_comparative_remediate_manual_execute"
        ],
        "average_score": 0,
        "consistency_score": 0,
        "reliability_score": 0,
        "strengths": "None identifiable. Catastrophic failure across evaluation.",
        "weaknesses": "Complete failure in 2/3 scenarios (67% failure rate). Scores 0.0 in participated scenarios. Non-functional for Kubernetes troubleshooting. Fundamental reliability issues. Unsuitable for any production use. Highest failure rate of all models tested.",
        "production_readiness": "avoid"
      }
    },
    "overall_assessment": {
      "winner": "vercel_claude-haiku-4-5-20251001",
      "rationale": "Claude-Haiku emerges as the clear overall winner based on exceptional cross-scenario reliability (98% reliability score), zero catastrophic failures, and the most consistent performance pattern. While Gemini-2.5-Flash wins 2/3 individual scenarios, its complete failure in manual_execute (0.189 score) represents a critical production risk that disqualifies it from top position. Claude-Haiku's scores (0.89, 0.92, 0.906) demonstrate it's the only model that reliably performs at >0.85 level across ALL scenarios. With minimal variance (σ=0.015) and sub-35s response times universally, it provides the operational reliability required for production Kubernetes troubleshooting. The consistency principle is critical here: a model you can trust 100% of the time at 0.90 quality beats one that's 0.93 quality 67% of the time but catastrophically fails 33% of the time. For production deployments where reliability is paramount, Claude-Haiku's proven consistency across diverse workflow patterns (automatic execution, manual analysis, manual execution) makes it the safest and most effective choice.",
      "reliability_ranking": [
        {
          "model": "vercel_claude-haiku-4-5-20251001",
          "reliability_score": 0.98,
          "reliability_notes": "100% participation, 100% success rate (all scores >0.85), 98% consistency. Zero failures."
        },
        {
          "model": "vercel_grok-4",
          "reliability_score": 0.99,
          "reliability_notes": "100% participation, 100% success rate, 99% consistency. Slightly lower scores but extremely reliable."
        },
        {
          "model": "vercel_claude-sonnet-4-5-20250929",
          "reliability_score": 0.97,
          "reliability_notes": "100% participation, 100% success rate, 97% consistency. Solid mid-tier reliability."
        },
        {
          "model": "vercel_gpt-5",
          "reliability_score": 0.97,
          "reliability_notes": "100% participation, 100% success rate, 97% consistency. Latency concerns but no failures."
        },
        {
          "model": "vercel_gpt-5-pro",
          "reliability_score": 0.93,
          "reliability_notes": "100% participation, 100% success rate, but catastrophic latency (22+ min) makes it production-unsuitable."
        },
        {
          "model": "vercel_deepseek-reasoner",
          "reliability_score": 0.89,
          "reliability_notes": "100% participation, 100% success rate, but infrastructure validation failures and inefficiency."
        },
        {
          "model": "vercel_grok-4-fast-reasoning",
          "reliability_score": 0.86,
          "reliability_notes": "100% participation, 100% success rate, 95% consistency. Fast and reliable."
        },
        {
          "model": "vercel_gemini-2.5-flash",
          "reliability_score": 0.86,
          "reliability_notes": "100% participation, 67% success rate (catastrophic failure in 1/3 scenarios), 43% consistency."
        },
        {
          "model": "vercel_gemini-2.5-pro",
          "reliability_score": 0.5,
          "reliability_notes": "67% participation (failed 1/3 scenarios), format failures, 75% consistency."
        },
        {
          "model": "vercel_mistral-large-latest",
          "reliability_score": 0,
          "reliability_notes": "33% participation (failed 2/3 scenarios), 0% success rate. Complete failure."
        }
      ],
      "production_recommendations": {
        "primary": "vercel_claude-haiku-4-5-20251001 - Most reliable choice with 98% reliability score, zero failures, consistent 0.89+ performance across all scenarios, and optimal balance of speed (17-33s), quality, and operational stability. Safe for all production Kubernetes troubleshooting workflows.",
        "secondary": "vercel_claude-sonnet-4-5-20250929 - Solid alternative with 97% reliability, 100% participation, and no catastrophic failures. Slightly lower scores but proven consistency. Good choice when Haiku is unavailable or for workloads requiring additional analytical depth.",
        "avoid": [
          "vercel_mistral-large-latest (0% reliability, 67% failure rate)",
          "vercel_gemini-2.5-pro (50% reliability, format failures, scenario failures)",
          "vercel_gpt-5-pro (catastrophic latency: 22+ minutes, production-blocking)"
        ],
        "specialized_use": {
          "time_critical_incidents_sub_20s": "vercel_grok-4-fast-reasoning - Optimal for urgent incidents requiring sub-20s response with 3-tool-call efficiency, though slightly less thorough than Haiku.",
          "peak_efficiency_when_stable": "vercel_gemini-2.5-flash - When workflow conditions are known stable (automatic execution, manual analysis), delivers exceptional speed and quality, but avoid for manual_execute workflows due to tool integration bug.",
          "cost_optimization_non_critical": "vercel_gpt-5 - Good diagnostic quality for non-urgent workloads where 49s+ latency is acceptable, but unsuitable for incident response."
        }
      },
      "key_insights": "This evaluation reveals three critical patterns: (1) Smaller optimized models (Haiku, Flash) dramatically outperform larger 'pro' variants, suggesting over-engineering hurts production performance. (2) Reliability trumps peak performance - Gemini-Flash's catastrophic failure in 33% of scenarios despite winning 67% disqualifies it from primary production use. (3) Extreme failure rates for premium models (Mistral 67% failure, GPT-5-Pro 22-minute latency, Gemini-Pro format failures) indicate that pricing/branding doesn't correlate with reliability. (4) The 'reasoning' approach (DeepSeek) shows over-investigation without quality gains, consuming 58K tokens unnecessarily. (5) Claude-Haiku's consistent 0.89-0.92 performance across ALL scenarios represents the gold standard for production Kubernetes automation - reliability and consistency matter more than occasional peak performance. For production deployments, only 4 models are truly suitable: Claude-Haiku (primary), Claude-Sonnet (secondary), Grok-4-Fast (time-critical), and Grok-4 (backup). All others show critical failure patterns that pose unacceptable operational risks."
    }
  },
  "results": [
    {
      "key": "remediation_comparative_remediate_automatic_analyze_execute",
      "score": 0.9,
      "comment": "This OOM troubleshooting scenario clearly demonstrates the importance of balancing diagnostic quality with operational efficiency. The top performers (Gemini Flash, Claude Haiku, Grok-4-Fast) achieved excellent root cause identification while maintaining fast response times and efficient token usage - critical for production incident response. A key pattern emerged: models that correctly identified the immutable nature of pod resources and validated commands with dry-run before execution showed higher reliability. The scenario also revealed significant reliability issues: response format failures (Gemini Pro), infrastructure validation failures (DeepSeek), extreme latency (GPT-5-Pro at 22+ minutes), and complete non-functionality (Mistral). For this straightforward OOM issue, simpler/faster models (Gemini Flash, Claude Haiku) outperformed more complex models, suggesting that excessive reasoning can hurt efficiency without proportional quality gains. Caching provided marginal benefits but couldn't overcome fundamental performance issues. The clear winner pattern shows that for production Kubernetes troubleshooting, sub-minute response times with >0.90 confidence and proper validation workflows represent the gold standard. Models exceeding 2-3 minutes response time are unsuitable for urgent incident response regardless of diagnostic quality.",
      "confidence": 0.9,
      "modelRankings": [
        {
          "rank": 1,
          "model": "vercel_gemini-2.5-flash",
          "score": 0.9
        },
        {
          "rank": 2,
          "model": "vercel_claude-haiku-4-5-20251001",
          "score": 0.89
        },
        {
          "rank": 3,
          "model": "vercel_grok-4-fast-reasoning",
          "score": 0.89
        },
        {
          "rank": 4,
          "model": "vercel_claude-sonnet-4-5-20250929",
          "score": 0.85
        },
        {
          "rank": 5,
          "model": "vercel_grok-4",
          "score": 0.83
        },
        {
          "rank": 6,
          "model": "vercel_gpt-5",
          "score": 0.8
        },
        {
          "rank": 7,
          "model": "vercel_gpt-5-pro",
          "score": 0.74
        },
        {
          "rank": 8,
          "model": "vercel_gemini-2.5-pro",
          "score": 0.68
        },
        {
          "rank": 9,
          "model": "vercel_deepseek-reasoner",
          "score": 0.67
        },
        {
          "rank": 10,
          "model": "vercel_mistral-large-latest",
          "score": 0
        }
      ],
      "bestModel": "vercel_gemini-2.5-flash",
      "modelCount": 10
    },
    {
      "key": "remediation_comparative_remediate_manual_analyze",
      "score": 0.93,
      "comment": "This scenario reveals clear performance tiers among models. Gemini-2.5-Flash emerges as the winner by optimizing the diagnostic path (only 4 tool calls) while maintaining quality, demonstrating that efficiency and accuracy aren't mutually exclusive. Claude-Haiku follows closely with superior communication clarity. The top tier (Flash, Haiku, Sonnet) all complete in under 35 seconds with accurate diagnoses. Mid-tier models (GPT-5, Grok-4, DeepSeek) provide good quality but suffer from efficiency issues with 47-55s durations and higher token usage. The bottom tier shows critical failures: GPT-5-Pro's 9-minute timeout is production-blocking, Gemini-2.5-Pro's JSON format failure breaks workflow automation, and Mistral's complete failure indicates fundamental reliability issues. For K8s troubleshooting, the data strongly favors models that balance speed (sub-20s), efficient tool usage (4-6 calls), and diagnostic accuracy. Cache utilization helps but doesn't compensate for poor diagnostic paths. The scenario also exposes that 'pro' or 'reasoning' labels don't guarantee better performance—optimization matters more than model size. For production K8s automation, only the top 5 models are viable, with Gemini-2.5-Flash offering the best speed-to-quality ratio.",
      "confidence": 0.9,
      "modelRankings": [
        {
          "rank": 1,
          "model": "vercel_gemini-2.5-flash",
          "score": 0.93
        },
        {
          "rank": 2,
          "model": "vercel_claude-haiku-4-5-20251001",
          "score": 0.92
        },
        {
          "rank": 3,
          "model": "vercel_claude-sonnet-4-5-20250929",
          "score": 0.85
        },
        {
          "rank": 4,
          "model": "vercel_gpt-5",
          "score": 0.84
        },
        {
          "rank": 5,
          "model": "vercel_grok-4",
          "score": 0.84
        },
        {
          "rank": 6,
          "model": "vercel_deepseek-reasoner",
          "score": 0.82
        },
        {
          "rank": 7,
          "model": "vercel_grok-4-fast-reasoning",
          "score": 0.81
        },
        {
          "rank": 8,
          "model": "vercel_gpt-5-pro",
          "score": 0.7
        },
        {
          "rank": 9,
          "model": "vercel_gemini-2.5-pro",
          "score": 0.53
        },
        {
          "rank": 10,
          "model": "vercel_mistral-large-latest",
          "score": 0
        }
      ],
      "bestModel": "vercel_gemini-2.5-flash",
      "modelCount": 10
    },
    {
      "key": "remediation_comparative_remediate_manual_execute",
      "score": 0.906,
      "comment": "This scenario reveals critical tradeoffs between diagnostic depth and operational speed. Claude Haiku emerges as the best all-rounder, balancing comprehensive analysis with reasonable performance. Grok-4-fast-reasoning demonstrates that efficient tool usage (3 calls vs 9-10) can deliver strong results with 40-80% faster response times, making it ideal for time-sensitive production incidents. The GPT-5 family shows excellent analytical depth but suffers from unacceptable latency (49s-626s), suggesting these models prioritize thoroughness over speed. DeepSeek's reasoning approach appears to over-investigate (11 iterations, 58K tokens) without commensurate quality gains. Most critically, Gemini-2.5-flash's complete failure to execute any tool calls highlights that not all models properly integrate with Kubernetes tooling - raw speed means nothing without functional capability. For production Kubernetes troubleshooting, Claude Haiku offers the best balance, while Grok-4-fast-reasoning is optimal when sub-20s response times are critical and slightly less detail is acceptable.",
      "confidence": 0.9,
      "modelRankings": [
        {
          "rank": 1,
          "model": "vercel_claude-haiku-4-5-20251001",
          "score": 0.906
        },
        {
          "rank": 2,
          "model": "vercel_grok-4-fast-reasoning",
          "score": 0.887
        },
        {
          "rank": 3,
          "model": "vercel_claude-sonnet-4-5-20250929",
          "score": 0.822
        },
        {
          "rank": 4,
          "model": "vercel_grok-4",
          "score": 0.822
        },
        {
          "rank": 5,
          "model": "vercel_gpt-5",
          "score": 0.812
        },
        {
          "rank": 6,
          "model": "vercel_gpt-5-pro",
          "score": 0.728
        },
        {
          "rank": 7,
          "model": "vercel_deepseek-reasoner",
          "score": 0.725
        },
        {
          "rank": 8,
          "model": "vercel_gemini-2.5-flash",
          "score": 0.189
        }
      ],
      "bestModel": "vercel_claude-haiku-4-5-20251001",
      "modelCount": 8
    }
  ],
  "summary": {
    "totalDatasets": 36,
    "availableModels": [
      "vercel_claude-haiku-4-5-20251001_2025-10-15",
      "vercel_claude-sonnet-4-5-20250929_2025-10-13",
      "vercel_deepseek-reasoner_2025-10-13",
      "vercel_gemini-2.5-flash_2025-10-14",
      "vercel_gemini-2.5-pro_2025-10-14",
      "vercel_gpt-5-pro_2025-10-14",
      "vercel_gpt-5_2025-10-14",
      "vercel_grok-4-fast-reasoning_2025-10-14",
      "vercel_grok-4_2025-10-14",
      "vercel_mistral-large-latest_2025-10-14"
    ],
    "scenariosWithMultipleModels": 3,
    "interactionTypes": [
      "automatic_analyze_execute",
      "manual_analyze",
      "manual_execute"
    ]
  }
}