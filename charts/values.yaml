# Deployment method configuration
deployment:
  method: standard                  # Deployment method: "standard" (K8s resources) or "toolhive" (MCPServer CRD)

# Application image configuration
image:
  repository: ghcr.io/vfarcic/dot-ai  # Container image repository
  tag: "0.125.0"                     # Container image tag - set by CI pipeline during release

# Resource configuration
resources:
  requests:
    memory: "512Mi"  # Minimum memory required
    cpu: "200m"      # Minimum CPU required
  limits:
    memory: "2Gi"    # Maximum memory allowed
    cpu: "1000m"     # Maximum CPU allowed

# Secrets configuration
secrets:
  name: dot-ai-secrets              # Name of the Kubernetes Secret resource
  anthropic:
    keyName: anthropic-api-key      # Key name within the secret
    apiKey: ""                      # API key value (only needed if chart should create the secret)
  openai:
    keyName: openai-api-key         # Key name within the secret
    apiKey: ""                      # API key value (only needed if chart should create the secret)
  google:
    keyName: google-api-key         # Key name within the secret
    apiKey: ""                      # API key value (only needed if chart should create the secret)
  xai:
    keyName: xai-api-key            # Key name within the secret
    apiKey: ""                      # API key value (only needed if chart should create the secret)
  mistral:
    keyName: mistral-api-key        # Key name within the secret
    apiKey: ""                      # API key value (only needed if chart should create the secret)
  deepseek:
    keyName: deepseek-api-key       # Key name within the secret
    apiKey: ""                      # API key value (only needed if chart should create the secret)
  customLlm:
    keyName: custom-llm-api-key     # Key name within the secret for custom LLM endpoint
    apiKey: ""                      # API key value (only needed if chart should create the secret)
  customEmbeddings:
    keyName: custom-embeddings-api-key  # Key name within the secret for custom embeddings endpoint
    apiKey: ""                      # API key value (only needed if chart should create the secret)

# ServiceAccount configuration
serviceAccount:
  create: true                      # Create a ServiceAccount
  name: ""                          # ServiceAccount name override (generated if empty)

# Ingress configuration
ingress:
  enabled: false                    # Create Ingress resource
  className: nginx                  # Ingress class name
  host: dot-ai.127.0.0.1.nip.io     # Ingress hostname
  # Annotations required for HTTP transport with SSE (Server-Sent Events)
  # If using different className, update annotations for your ingress controller:
  # - Traefik: traefik.ingress.kubernetes.io/service.sticky.cookie.httponly: "true"
  # - HAProxy: haproxy.org/timeout-http-request: "3600s"
  # - AWS ALB: alb.ingress.kubernetes.io/target-group-attributes: idle_timeout.timeout_seconds=3600
  annotations:
    nginx.ingress.kubernetes.io/proxy-read-timeout: "3600"    # Allow long-running SSE connections
    nginx.ingress.kubernetes.io/proxy-send-timeout: "3600"    # Allow long-running SSE connections
    nginx.ingress.kubernetes.io/proxy-buffering: "off"        # Disable buffering for real-time streaming
    nginx.ingress.kubernetes.io/proxy-request-buffering: "off" # Disable request buffering for real-time streaming
  tls:
    enabled: false                  # Enable TLS/HTTPS
    secretName: ""                  # TLS secret name (generated if empty when enabled)

# AI Provider configuration
ai:
  provider: anthropic               # AI provider type (anthropic, openai, google, xai, mistral, deepseek)
  model: ""                         # Optional: model override (e.g., "llama3.1:70b", "gpt-4o")

  # Custom endpoint configuration for self-hosted or alternative SaaS providers (PRD #194)
  customEndpoint:
    enabled: false                  # Enable custom endpoint
    baseURL: ""                     # Custom LLM endpoint URL (e.g., "http://ollama-service:11434/v1")
    embeddingsBaseURL: ""           # Optional: custom embeddings endpoint URL (if different from LLM endpoint)

  # Examples (commented out):
  # Example 1: Ollama (self-hosted)
  # ai:
  #   provider: openai
  #   model: "llama3.1:70b"
  #   customEndpoint:
  #     enabled: true
  #     baseURL: "http://ollama-service:11434/v1"
  #
  # Example 2: Azure OpenAI (SaaS)
  # ai:
  #   provider: openai
  #   model: "gpt-4o"
  #   customEndpoint:
  #     enabled: true
  #     baseURL: "https://YOUR_RESOURCE.openai.azure.com/openai/deployments/YOUR_DEPLOYMENT"
  #
  # Example 3: vLLM (self-hosted)
  # ai:
  #   provider: openai
  #   model: "meta-llama/Llama-3.1-70B-Instruct"
  #   customEndpoint:
  #     enabled: true
  #     baseURL: "http://vllm-service:8000/v1"
  #
  # Note: Custom endpoints must support OpenAI-compatible API and models must
  # support 8K+ output tokens for reliable YAML generation.

# Additional environment variables (optional)
# Use this to add any custom environment variables to the MCP server
# Example use cases: tracing configuration, custom integrations, feature flags
extraEnv: []
  # - name: OTEL_TRACING_ENABLED
  #   value: "true"
  # - name: OTEL_EXPORTER_OTLP_ENDPOINT
  #   value: "http://jaeger-collector:4318/v1/traces"
  # - name: OTEL_SERVICE_NAME
  #   value: "dot-ai-mcp-production"

# Qdrant Vector Database
qdrant:
  enabled: true                     # Deploy Qdrant as dependency (false = use external)
  image:
    repository: qdrant/qdrant       # Qdrant image repository
    tag: v1.15.5                    # Qdrant image tag
  external:
    url: ""                         # External Qdrant URL (required when enabled=false)
    apiKey: ""                      # External Qdrant API key (optional)