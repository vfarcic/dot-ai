#!/usr/bin/env npx tsx

/**
 * Evaluation Runner for Multi-Model Comparative Analysis
 * 
 * Runs comparative evaluation on available datasets from multiple models
 * Automatically detects and evaluates both remediation and recommendation datasets
 */

import { RemediationComparativeEvaluator } from './evaluators/remediation-comparative.js';
import { RecommendationComparativeEvaluator } from './evaluators/recommendation-comparative.js';
import { CapabilityComparativeEvaluator } from './evaluators/capability-comparative.js';
import { PatternComparativeEvaluator } from './evaluators/pattern-comparative.js';
import { PolicyComparativeEvaluator } from './evaluators/policy-comparative.js';
import { readdir } from 'fs/promises';

const EVALUATOR_CONFIG = {
  remediation: { 
    evaluator: RemediationComparativeEvaluator, 
    prefix: 'remediate_', 
    title: 'Remediation AI Model Comparison Report' 
  },
  recommendation: { 
    evaluator: RecommendationComparativeEvaluator, 
    prefix: 'recommend_', 
    title: 'Recommendation AI Model Comparison Report' 
  },
  capability: { 
    evaluator: CapabilityComparativeEvaluator, 
    prefix: 'capability_', 
    title: 'Capability AI Model Comparison Report' 
  },
  pattern: { 
    evaluator: PatternComparativeEvaluator, 
    prefix: 'pattern_', 
    title: 'Pattern AI Model Comparison Report' 
  },
  policy: { 
    evaluator: PolicyComparativeEvaluator, 
    prefix: 'policy_', 
    title: 'Policy AI Model Comparison Report' 
  }
} as const;

type EvaluationType = keyof typeof EVALUATOR_CONFIG;

function generateMarkdownReport(results: any[], stats: any, evaluationType: EvaluationType): string {
  const timestamp = new Date().toISOString();
  
  // Calculate overall statistics
  const modelWins = new Map<string, number>();
  const modelScores = new Map<string, number[]>();
  
  results.forEach(result => {
    const winner = result.bestModel;
    modelWins.set(winner, (modelWins.get(winner) || 0) + 1);
    
    // Collect all scores for each model
    if (result.modelRankings) {
      result.modelRankings.forEach((ranking: any) => {
        if (!modelScores.has(ranking.model)) {
          modelScores.set(ranking.model, []);
        }
        modelScores.get(ranking.model)!.push(ranking.score);
      });
    }
  });
  
  const sortedWins = Array.from(modelWins.entries()).sort((a, b) => b[1] - a[1]);
  
  // Calculate average scores
  const modelAverages = new Map<string, number>();
  modelScores.forEach((scores, model) => {
    const avg = scores.reduce((a, b) => a + b, 0) / scores.length;
    modelAverages.set(model, Math.round(avg * 1000) / 1000);
  });
  
  const reportTitle = EVALUATOR_CONFIG[evaluationType].title;
  
  return `# ${reportTitle}

**Generated**: ${timestamp}  
**Scenarios Analyzed**: ${results.length}  
**Models Evaluated**: ${stats.availableModels.length}  
**Total Datasets**: ${stats.totalDatasets}

## Executive Summary

### üèÜ Overall Winners
${sortedWins.map(([model, wins]) => `- **${model}**: ${wins} scenario${wins > 1 ? 's' : ''} won`).join('\n')}

### üìä Model Performance Overview

| Model | Avg Score | Scenarios Won | Performance Notes |
|-------|-----------|---------------|-------------------|
${Array.from(modelAverages.entries())
  .sort((a, b) => b[1] - a[1])
  .map(([model, avgScore]) => {
    const wins = modelWins.get(model) || 0;
    const performance = wins > 0 ? 'üü¢ Strong' : wins === 0 && avgScore > 0.8 ? 'üü° Good' : 'üî¥ Weak';
    return `| ${model} | ${avgScore} | ${wins} | ${performance} |`;
  }).join('\n')}

## Detailed Scenario Results

${results.map((result, index) => {
  const scenarioTitle = result.key.replace(/_/g, ' ').replace(/(remediation|recommendation) comparative /, '').toUpperCase();
  
  return `### ${index + 1}. ${scenarioTitle}

**Winner**: ${result.bestModel} (Score: ${result.score})  
**Models Compared**: ${result.modelCount}  
**Confidence**: ${result.confidence ? Math.round(result.confidence * 100) : 0}%

#### Rankings
${result.modelRankings ? result.modelRankings.map((rank: any) => 
  `${rank.rank}. **${rank.model}** - ${rank.score}`
).join('\n') : 'No detailed rankings available'}

#### Analysis
${result.comment}

---`;
}).join('\n\n')}

## Model Selection Guide
${sortedWins.slice(0, 3).map(([model], index) => {
  const score = modelAverages.get(model) || 0;
  const useCase = index === 0 ? 'Primary choice for most scenarios' : 
                  index === 1 ? 'Good alternative with different strengths' : 
                  'Specialized use cases';
  return `- **${model}** (Avg: ${score}): ${useCase}`;
}).join('\n')}

---

## Report Attribution

Report generated by DevOps AI Toolkit Comparative Evaluation System
`;
}

function loadModelMetadata() {
  try {
    const fs = require('fs');
    const path = require('path');
    const metadataPath = path.join(__dirname, 'model-metadata.json');
    
    if (!fs.existsSync(metadataPath)) {
      console.error('‚ùå Model metadata file not found');
      console.error('üìä Pricing and capabilities data required for cost analysis');
      console.error('');
      console.error('üîÑ To create model metadata, run:');
      console.error('   /update-model-metadata');
      console.error('');
      process.exit(1);
    }
    
    const metadata = JSON.parse(fs.readFileSync(metadataPath, 'utf8'));
    
    // Check if metadata is older than 30 days
    const metadataAge = Date.now() - new Date(metadata.lastUpdated).getTime();
    const thirtyDays = 30 * 24 * 60 * 60 * 1000;
    
    if (metadataAge > thirtyDays) {
      console.error('‚ùå Model metadata is over 30 days old (last updated: ' + metadata.lastUpdated + ')');
      console.error('üìä Pricing and capabilities data may be outdated, affecting cost analysis accuracy');
      console.error('');
      console.error('üîÑ To update model metadata, run:');
      console.error('   /update-model-metadata');
      console.error('');
      process.exit(1);
    }
    
    console.log('‚úÖ Model metadata loaded (updated: ' + metadata.lastUpdated + ')');
    return metadata;
  } catch (error) {
    console.error('‚ùå Failed to load model metadata:', error.message);
    console.error('üîÑ To create model metadata, run: /update-model-metadata');
    process.exit(1);
  }
}

function generateJsonReport(results: any[], stats: any, evaluationType: EvaluationType, modelMetadata: any) {
  const timestamp = new Date().toISOString();
  
  return {
    metadata: {
      reportType: 'comparative-evaluation',
      evaluationType: evaluationType,
      generated: timestamp,
      scenariosAnalyzed: results.length,
      modelsEvaluated: stats.availableModels.length,
      totalDatasets: stats.totalDatasets,
      tool: EVALUATOR_CONFIG[evaluationType].title
    },
    modelMetadata: modelMetadata.models,
    results: results,
    summary: stats
  };
}

async function detectAvailableDatasets(datasetsDir: string): Promise<Record<EvaluationType, boolean>> {
  try {
    const files = await readdir(datasetsDir);
    const result: Record<string, boolean> = {};
    
    for (const [type, config] of Object.entries(EVALUATOR_CONFIG)) {
      result[type] = files.some(file => file.startsWith(config.prefix));
    }
    
    return result as Record<EvaluationType, boolean>;
  } catch (error) {
    console.warn('Could not read datasets directory, assuming no datasets available');
    const result: Record<string, boolean> = {};
    for (const type of Object.keys(EVALUATOR_CONFIG)) {
      result[type] = false;
    }
    return result as Record<EvaluationType, boolean>;
  }
}

async function runEvaluation(evaluatorType: EvaluationType, datasetsDir: string, modelMetadata: any) {
  const EvaluatorClass = EVALUATOR_CONFIG[evaluatorType].evaluator;
  const evaluator = new EvaluatorClass(datasetsDir);
  
  console.log(`\nüî¨ Starting ${evaluatorType.charAt(0).toUpperCase() + evaluatorType.slice(1)} Evaluation\n`);
  
  // Show dataset stats
  console.log('üìä Dataset Analysis:');
  const stats = evaluator.getDatasetStats();
  console.log(`- Total datasets: ${stats.totalDatasets}`);
  console.log(`- Available models: ${stats.availableModels.join(', ')}`);
  console.log(`- Scenarios with multiple models: ${stats.scenariosWithMultipleModels}`);
  console.log(`- Interaction types: ${stats.interactionTypes.join(', ')}`);
  console.log();
  
  // Show evaluation phases
  console.log('üéØ Evaluation Phases:');
  const phases = evaluator.getEvaluationPhases();
  phases.forEach(phase => {
    console.log(`- ${phase.phase}: ${phase.description}`);
    console.log(`  Models: ${phase.availableModels.join(', ')}`);
    console.log(`  Scenarios: ${phase.scenarioCount}`);
    console.log();
  });
  
  // Run comparative evaluation on all scenarios
  console.log('üöÄ Running Comparative Evaluation...\n');
  
  const results = await evaluator.evaluateAllScenarios();
  
  console.log(`‚úÖ ${evaluatorType.charAt(0).toUpperCase() + evaluatorType.slice(1)} Evaluation Complete! Analyzed ${results.length} scenarios\n`);
  
  // Generate dual-format reports
  const reportContent = generateMarkdownReport(results, stats, evaluatorType);
  const jsonResults = generateJsonReport(results, stats, evaluatorType, modelMetadata);
  
  // Save reports to files
  const dateStamp = new Date().toISOString().split('T')[0];
  const markdownPath = `./eval/reports/${evaluatorType}-evaluation-${dateStamp}.md`;
  const jsonPath = `./eval/reports/${evaluatorType}-results-${dateStamp}.json`;
  const reportDir = './eval/reports';
  
  // Ensure report directory exists
  const fs = await import('fs');
  if (!fs.existsSync(reportDir)) {
    fs.mkdirSync(reportDir, { recursive: true });
  }
  
  fs.writeFileSync(markdownPath, reportContent);
  fs.writeFileSync(jsonPath, JSON.stringify(jsonResults, null, 2));
  
  console.log(`üìä ${evaluatorType.charAt(0).toUpperCase() + evaluatorType.slice(1)} reports generated:`);
  console.log(`   üìù Markdown: ${markdownPath}`);
  console.log(`   üìÑ JSON: ${jsonPath}`);
  
  // Brief console summary
  console.log(`üèÜ ${evaluatorType.charAt(0).toUpperCase() + evaluatorType.slice(1)} Results:`);
  results.forEach((result, index) => {
    console.log(`   ${index + 1}. ${result.key}: ${result.bestModel} (${result.score})`);
  });
  
  return results;
}

async function main() {
  console.log('üî¨ Starting Multi-Model Comparative Evaluation\n');
  
  // Check model metadata freshness before starting any evaluation work
  const modelMetadata = loadModelMetadata();
  
  const datasetsDir = './eval/datasets';
  const availableDatasets = await detectAvailableDatasets(datasetsDir);
  
  console.log('üîç Dataset Detection:');
  for (const [type, available] of Object.entries(availableDatasets)) {
    console.log(`- ${type.charAt(0).toUpperCase() + type.slice(1)} datasets: ${available ? '‚úÖ' : '‚ùå'}`);
  }
  
  const hasAnyDatasets = Object.values(availableDatasets).some(Boolean);
  if (!hasAnyDatasets) {
    console.error('‚ùå No evaluation datasets found. Please run integration tests first to generate datasets.');
    process.exit(1);
  }
  
  try {
    const allResults = [];
    
    for (const [type, available] of Object.entries(availableDatasets)) {
      if (available) {
        const results = await runEvaluation(type as EvaluationType, datasetsDir, modelMetadata);
        allResults.push(...results);
      }
    }
    
    console.log(`\nüéâ All Evaluations Complete! Total scenarios analyzed: ${allResults.length}`);
    console.log(`üìÅ Check ./eval/reports/ for detailed analysis reports\n`);
    
  } catch (error) {
    console.error('‚ùå Evaluation failed:', error);
    process.exit(1);
  }
}

// Run if this file is executed directly
if (require.main === module) {
  main().catch(console.error);
}