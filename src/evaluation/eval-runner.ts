#!/usr/bin/env npx tsx

/**
 * Evaluation Runner for Multi-Model Comparative Analysis
 * 
 * Runs comparative evaluation on remediation datasets from multiple models
 */

import { RemediationComparativeEvaluator } from './evaluators/remediation-comparative.js';

function generateMarkdownReport(results: any[], stats: any): string {
  const timestamp = new Date().toISOString();
  
  // Calculate overall statistics
  const modelWins = new Map<string, number>();
  const modelScores = new Map<string, number[]>();
  
  results.forEach(result => {
    const winner = result.bestModel;
    modelWins.set(winner, (modelWins.get(winner) || 0) + 1);
    
    // Collect all scores for each model
    if (result.modelRankings) {
      result.modelRankings.forEach((ranking: any) => {
        if (!modelScores.has(ranking.model)) {
          modelScores.set(ranking.model, []);
        }
        modelScores.get(ranking.model)!.push(ranking.score);
      });
    }
  });
  
  const sortedWins = Array.from(modelWins.entries()).sort((a, b) => b[1] - a[1]);
  
  // Calculate average scores
  const modelAverages = new Map<string, number>();
  modelScores.forEach((scores, model) => {
    const avg = scores.reduce((a, b) => a + b, 0) / scores.length;
    modelAverages.set(model, Math.round(avg * 1000) / 1000);
  });
  
  return `# Remediation AI Model Comparison Report

**Generated**: ${timestamp}  
**Scenarios Analyzed**: ${results.length}  
**Models Evaluated**: ${stats.availableModels.length}  
**Total Datasets**: ${stats.totalDatasets}

## Executive Summary

### üèÜ Overall Winners
${sortedWins.map(([model, wins]) => `- **${model}**: ${wins} scenario${wins > 1 ? 's' : ''} won`).join('\n')}

### üìä Model Performance Overview
| Model | Avg Score | Scenarios Won | Performance Notes |
|-------|-----------|---------------|-------------------|
${Array.from(modelAverages.entries())
  .sort((a, b) => b[1] - a[1])
  .map(([model, avgScore]) => {
    const wins = modelWins.get(model) || 0;
    const performance = wins > 0 ? 'üü¢ Strong' : wins === 0 && avgScore > 0.8 ? 'üü° Good' : 'üî¥ Weak';
    return `| ${model} | ${avgScore} | ${wins} | ${performance} |`;
  }).join('\n')}

## Detailed Scenario Results

${results.map((result, index) => {
  const scenarioTitle = result.key.replace(/_/g, ' ').replace(/remediation comparative /, '').toUpperCase();
  
  return `### ${index + 1}. ${scenarioTitle}

**Winner**: ${result.bestModel} (Score: ${result.score})  
**Models Compared**: ${result.modelCount}  
**Confidence**: ${Math.round(result.confidence * 100)}%

#### Rankings
${result.modelRankings ? result.modelRankings.map((rank: any) => 
  `${rank.rank}. **${rank.model}** - ${rank.score}`
).join('\n') : 'No detailed rankings available'}

#### Analysis
${result.comment}

---`;
}).join('\n\n')}

## Key Insights & Recommendations

### Performance Patterns
- **Speed vs Quality Trade-off**: Higher quality models often require longer processing times
- **Cache Utilization**: Models with effective caching showed significantly better performance
- **Resource Efficiency**: Token usage patterns varied significantly between models

### Production Recommendations
1. **Real-time Troubleshooting**: Use the fastest performing model with acceptable accuracy
2. **Automated Remediation**: Balance speed and reliability for production environments  
3. **Complex Analysis**: Consider using higher-scoring models when time permits detailed investigation

### Model Selection Guide
${sortedWins.slice(0, 3).map(([model], index) => {
  const score = modelAverages.get(model) || 0;
  const useCase = index === 0 ? 'Primary choice for most scenarios' : 
                  index === 1 ? 'Good alternative with different strengths' : 
                  'Specialized use cases';
  return `- **${model}** (Avg: ${score}): ${useCase}`;
}).join('\n')}

---
*Report generated by DevOps AI Toolkit Comparative Evaluation System*
`;
}

async function main() {
  console.log('üî¨ Starting Multi-Model Comparative Evaluation\n');
  
  const evaluator = new RemediationComparativeEvaluator('./eval/datasets');
  
  // Show dataset stats
  console.log('üìä Dataset Analysis:');
  const stats = evaluator.getDatasetStats();
  console.log(`- Total datasets: ${stats.totalDatasets}`);
  console.log(`- Available models: ${stats.availableModels.join(', ')}`);
  console.log(`- Scenarios with multiple models: ${stats.scenariosWithMultipleModels}`);
  console.log(`- Interaction types: ${stats.interactionTypes.join(', ')}`);
  console.log();
  
  // Show evaluation phases
  console.log('üéØ Evaluation Phases:');
  const phases = evaluator.getEvaluationPhases();
  phases.forEach(phase => {
    console.log(`- ${phase.phase}: ${phase.description}`);
    console.log(`  Models: ${phase.availableModels.join(', ')}`);
    console.log(`  Scenarios: ${phase.scenarioCount}`);
    console.log();
  });
  
  // Run comparative evaluation on all scenarios
  console.log('üöÄ Running Comparative Evaluation...\n');
  
  try {
    const results = await evaluator.evaluateAllScenarios();
    
    console.log(`‚úÖ Evaluation Complete! Analyzed ${results.length} scenarios\n`);
    
    // Generate markdown report
    const reportContent = generateMarkdownReport(results, stats);
    
    // Save report to file
    const reportPath = `./eval/reports/comparative-evaluation-${new Date().toISOString().split('T')[0]}.md`;
    const reportDir = './eval/reports';
    
    // Ensure report directory exists
    const fs = await import('fs');
    if (!fs.existsSync(reportDir)) {
      fs.mkdirSync(reportDir, { recursive: true });
    }
    
    fs.writeFileSync(reportPath, reportContent);
    
    console.log(`üìä Markdown report generated: ${reportPath}`);
    console.log(`üìÅ Open the report to see detailed comparison analysis\n`);
    
    // Brief console summary
    console.log('üèÜ Quick Results:');
    results.forEach((result, index) => {
      console.log(`   ${index + 1}. ${result.key}: ${result.bestModel} (${result.score})`);
    });
    
  } catch (error) {
    console.error('‚ùå Evaluation failed:', error);
    process.exit(1);
  }
}

// Run if this file is executed directly
if (require.main === module) {
  main().catch(console.error);
}