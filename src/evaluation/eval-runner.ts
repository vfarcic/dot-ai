#!/usr/bin/env npx tsx

/**
 * Evaluation Runner for Multi-Model Comparative Analysis
 * 
 * Runs comparative evaluation on available datasets from multiple models
 * Automatically detects and evaluates both remediation and recommendation datasets
 */

import { RemediationComparativeEvaluator } from './evaluators/remediation-comparative.js';
import { RecommendationComparativeEvaluator } from './evaluators/recommendation-comparative.js';
import { CapabilityComparativeEvaluator } from './evaluators/capability-comparative.js';
import { readdir } from 'fs/promises';

function generateMarkdownReport(results: any[], stats: any, evaluationType: 'remediation' | 'recommendation' | 'capability'): string {
  const timestamp = new Date().toISOString();
  
  // Calculate overall statistics
  const modelWins = new Map<string, number>();
  const modelScores = new Map<string, number[]>();
  
  results.forEach(result => {
    const winner = result.bestModel;
    modelWins.set(winner, (modelWins.get(winner) || 0) + 1);
    
    // Collect all scores for each model
    if (result.modelRankings) {
      result.modelRankings.forEach((ranking: any) => {
        if (!modelScores.has(ranking.model)) {
          modelScores.set(ranking.model, []);
        }
        modelScores.get(ranking.model)!.push(ranking.score);
      });
    }
  });
  
  const sortedWins = Array.from(modelWins.entries()).sort((a, b) => b[1] - a[1]);
  
  // Calculate average scores
  const modelAverages = new Map<string, number>();
  modelScores.forEach((scores, model) => {
    const avg = scores.reduce((a, b) => a + b, 0) / scores.length;
    modelAverages.set(model, Math.round(avg * 1000) / 1000);
  });
  
  const reportTitle = evaluationType === 'remediation' ? 'Remediation AI Model Comparison Report' : 
    evaluationType === 'recommendation' ? 'Recommendation AI Model Comparison Report' : 
    'Capability AI Model Comparison Report';
  
  return `# ${reportTitle}

**Generated**: ${timestamp}  
**Scenarios Analyzed**: ${results.length}  
**Models Evaluated**: ${stats.availableModels.length}  
**Total Datasets**: ${stats.totalDatasets}

## Executive Summary

### üèÜ Overall Winners
${sortedWins.map(([model, wins]) => `- **${model}**: ${wins} scenario${wins > 1 ? 's' : ''} won`).join('\n')}

### üìä Model Performance Overview
| Model | Avg Score | Scenarios Won | Performance Notes |
|-------|-----------|---------------|-------------------|
${Array.from(modelAverages.entries())
  .sort((a, b) => b[1] - a[1])
  .map(([model, avgScore]) => {
    const wins = modelWins.get(model) || 0;
    const performance = wins > 0 ? 'üü¢ Strong' : wins === 0 && avgScore > 0.8 ? 'üü° Good' : 'üî¥ Weak';
    return `| ${model} | ${avgScore} | ${wins} | ${performance} |`;
  }).join('\n')}

## Detailed Scenario Results

${results.map((result, index) => {
  const scenarioTitle = result.key.replace(/_/g, ' ').replace(/(remediation|recommendation) comparative /, '').toUpperCase();
  
  return `### ${index + 1}. ${scenarioTitle}

**Winner**: ${result.bestModel} (Score: ${result.score})  
**Models Compared**: ${result.modelCount}  
**Confidence**: ${Math.round(result.confidence * 100)}%

#### Rankings
${result.modelRankings ? result.modelRankings.map((rank: any) => 
  `${rank.rank}. **${rank.model}** - ${rank.score}`
).join('\n') : 'No detailed rankings available'}

#### Analysis
${result.comment}

---`;
}).join('\n\n')}

## Model Selection Guide
${sortedWins.slice(0, 3).map(([model], index) => {
  const score = modelAverages.get(model) || 0;
  const useCase = index === 0 ? 'Primary choice for most scenarios' : 
                  index === 1 ? 'Good alternative with different strengths' : 
                  'Specialized use cases';
  return `- **${model}** (Avg: ${score}): ${useCase}`;
}).join('\n')}

---
*Report generated by DevOps AI Toolkit Comparative Evaluation System*
`;
}

async function detectAvailableDatasets(datasetsDir: string): Promise<{
  hasRemediation: boolean;
  hasRecommendation: boolean;
  hasCapability: boolean;
}> {
  try {
    const files = await readdir(datasetsDir);
    const hasRemediation = files.some(file => file.startsWith('remediate_'));
    const hasRecommendation = files.some(file => file.startsWith('recommend_'));
    const hasCapability = files.some(file => file.startsWith('capability_'));
    return { hasRemediation, hasRecommendation, hasCapability };
  } catch (error) {
    console.warn('Could not read datasets directory, assuming no datasets available');
    return { hasRemediation: false, hasRecommendation: false, hasCapability: false };
  }
}

async function runEvaluation(evaluatorType: 'remediation' | 'recommendation' | 'capability', datasetsDir: string) {
  const evaluator = evaluatorType === 'remediation' 
    ? new RemediationComparativeEvaluator(datasetsDir)
    : evaluatorType === 'recommendation'
    ? new RecommendationComparativeEvaluator(datasetsDir)
    : new CapabilityComparativeEvaluator(datasetsDir);
  
  console.log(`\nüî¨ Starting ${evaluatorType.charAt(0).toUpperCase() + evaluatorType.slice(1)} Evaluation\n`);
  
  // Show dataset stats
  console.log('üìä Dataset Analysis:');
  const stats = evaluator.getDatasetStats();
  console.log(`- Total datasets: ${stats.totalDatasets}`);
  console.log(`- Available models: ${stats.availableModels.join(', ')}`);
  console.log(`- Scenarios with multiple models: ${stats.scenariosWithMultipleModels}`);
  console.log(`- Interaction types: ${stats.interactionTypes.join(', ')}`);
  console.log();
  
  // Show evaluation phases
  console.log('üéØ Evaluation Phases:');
  const phases = evaluator.getEvaluationPhases();
  phases.forEach(phase => {
    console.log(`- ${phase.phase}: ${phase.description}`);
    console.log(`  Models: ${phase.availableModels.join(', ')}`);
    console.log(`  Scenarios: ${phase.scenarioCount}`);
    console.log();
  });
  
  // Run comparative evaluation on all scenarios
  console.log('üöÄ Running Comparative Evaluation...\n');
  
  const results = await evaluator.evaluateAllScenarios();
  
  console.log(`‚úÖ ${evaluatorType.charAt(0).toUpperCase() + evaluatorType.slice(1)} Evaluation Complete! Analyzed ${results.length} scenarios\n`);
  
  // Generate markdown report
  const reportContent = generateMarkdownReport(results, stats, evaluatorType);
  
  // Save report to file
  const reportPath = `./eval/reports/${evaluatorType}-evaluation-${new Date().toISOString().split('T')[0]}.md`;
  const reportDir = './eval/reports';
  
  // Ensure report directory exists
  const fs = await import('fs');
  if (!fs.existsSync(reportDir)) {
    fs.mkdirSync(reportDir, { recursive: true });
  }
  
  fs.writeFileSync(reportPath, reportContent);
  
  console.log(`üìä ${evaluatorType.charAt(0).toUpperCase() + evaluatorType.slice(1)} report generated: ${reportPath}`);
  
  // Brief console summary
  console.log(`üèÜ ${evaluatorType.charAt(0).toUpperCase() + evaluatorType.slice(1)} Results:`);
  results.forEach((result, index) => {
    console.log(`   ${index + 1}. ${result.key}: ${result.bestModel} (${result.score})`);
  });
  
  return results;
}

async function main() {
  console.log('üî¨ Starting Multi-Model Comparative Evaluation\n');
  
  const datasetsDir = './eval/datasets';
  const { hasRemediation, hasRecommendation, hasCapability } = await detectAvailableDatasets(datasetsDir);
  
  console.log('üîç Dataset Detection:');
  console.log(`- Remediation datasets: ${hasRemediation ? '‚úÖ' : '‚ùå'}`);
  console.log(`- Recommendation datasets: ${hasRecommendation ? '‚úÖ' : '‚ùå'}`);
  console.log(`- Capability datasets: ${hasCapability ? '‚úÖ' : '‚ùå'}`);
  
  if (!hasRemediation && !hasRecommendation && !hasCapability) {
    console.error('‚ùå No evaluation datasets found. Please run integration tests first to generate datasets.');
    process.exit(1);
  }
  
  try {
    const allResults = [];
    
    if (hasRemediation) {
      const remediationResults = await runEvaluation('remediation', datasetsDir);
      allResults.push(...remediationResults);
    }
    
    if (hasRecommendation) {
      const recommendationResults = await runEvaluation('recommendation', datasetsDir);  
      allResults.push(...recommendationResults);
    }
    
    if (hasCapability) {
      const capabilityResults = await runEvaluation('capability', datasetsDir);
      allResults.push(...capabilityResults);
    }
    
    console.log(`\nüéâ All Evaluations Complete! Total scenarios analyzed: ${allResults.length}`);
    console.log(`üìÅ Check ./eval/reports/ for detailed analysis reports\n`);
    
  } catch (error) {
    console.error('‚ùå Evaluation failed:', error);
    process.exit(1);
  }
}

// Run if this file is executed directly
if (require.main === module) {
  main().catch(console.error);
}